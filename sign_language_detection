import pickle
import cv2
import mediapipe as mp
import numpy as np
import time
from collections import deque

# Load trained model and label encoder
with open("sign_language_model.pkl", "rb") as f:
    model = pickle.load(f)
with open("label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

# Mediapipe Hand Detection
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.9)

# Open webcam
cap = cv2.VideoCapture(0)
print("Opening webcam for real-time sign language detection...")

# Buffer for stable predictions
word_buffer = deque(maxlen=5)
last_detected_time = time.time()

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(frame_rgb)

    data_aux = []
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            x_vals = [lm.x for lm in hand_landmarks.landmark]
            y_vals = [lm.y for lm in hand_landmarks.landmark]
            z_vals = [lm.z for lm in hand_landmarks.landmark]
            
            min_x, max_x = min(x_vals), max(x_vals)
            min_y, max_y = min(y_vals), max(y_vals)
            
            for i in range(21):
                normalized_x = (x_vals[i] - min_x) / (max_x - min_x)
                normalized_y = (y_vals[i] - min_y) / (max_y - min_y)
                normalized_z = z_vals[i]
                data_aux.extend([normalized_x, normalized_y, normalized_z])
            
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # Predict letter every 2 seconds
        if time.time() - last_detected_time >= 2 and len(data_aux) == 63:
            prediction = model.predict([data_aux])[0]
            detected_letter = label_encoder.inverse_transform([prediction])[0]
            word_buffer.append(detected_letter)
            last_detected_time = time.time()

    # Form stable word from buffer
    detected_word = "".join(word_buffer)
    cv2.putText(frame, f"Word: {detected_word}", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
    
    cv2.imshow("Sign Language Detection", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
